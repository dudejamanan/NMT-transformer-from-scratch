{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XkwyCtwkI_W",
        "outputId": "2282e62a-d8e0-4ca6-959e-2207987b8319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all the important libraries from pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "1-l7mjRIkLpv"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "xTNm8xRgkOwD"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HuggingFace libraries\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ],
      "metadata": {
        "id": "1qreMBf-kVV6"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pathlib\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "-mzDzSe-kV7m"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library of warnings\n",
        "import warnings\n",
        "#Library for progress bars in loops\n",
        "from tqdm import tqdm\n",
        "#typing\n",
        "from typing import Any"
      ],
      "metadata": {
        "id": "hGVYhtapkYav"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Input Embeddings\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # Dimension of embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multiply by sqrt(d_model) for normalization\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "9hrF5EZEkeB4"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Create matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
        "                             * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # shape: (1, seq_len, d_model)\n",
        "\n",
        "        # Register pe as buffer so it moves with model.to(device)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to input\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "tCWHnGutk23X"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Layer Normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        # scale (Î³) and shift (Î²) parameters â€” learnable\n",
        "        self.weight = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "\n",
        "        return self.weight * (x - mean) / (std + self.eps) + self.bias\n"
      ],
      "metadata": {
        "id": "kIKeTHQClA7F"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Feed Forward Layers\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.activation = nn.GELU()  # Better than ReLU for transformers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Rh_2eTUnlFjG"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention Block\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0, \"d_model must be divisible by number of heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        # Linear layers for q, k, v and final output\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Project and split into heads: (B, S, D) -> (B, H, S, d_k)\n",
        "        def shape(x):\n",
        "            return x.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        query = shape(self.w_q(q))\n",
        "        key   = shape(self.w_k(k))\n",
        "        value = shape(self.w_v(v))\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            # ensure mask shape: (B, 1, 1, S) so it broadcasts correctly\n",
        "            if mask.dim() == 3:\n",
        "                mask = mask.unsqueeze(1)\n",
        "            scores = scores.masked_fill(mask == 0, -1e4)  # stable for FP16\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = attn @ value  # -> (B, H, S, d_k)\n",
        "\n",
        "        # Combine heads back: (B, H, S, d_k) -> (B, S, D)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        return self.w_o(x)\n"
      ],
      "metadata": {
        "id": "Xe6Nvs5vlJJa"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Residual Connection\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormalization()  # Normalization before sublayer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # Apply norm â†’ sublayer â†’ dropout then residual add\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "UQiFwmOflNaA"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        self_attention_block: MultiHeadAttentionBlock,\n",
        "        feed_forward_block: FeedForwardBlock,\n",
        "        dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "\n",
        "        # Two residual connections: 1 for MHA, 1 for FFN\n",
        "        self.residual_connections = nn.ModuleList([\n",
        "            ResidualConnection(d_model, dropout),\n",
        "            ResidualConnection(d_model, dropout)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # Self-attention residual\n",
        "        x = self.residual_connections[0](\n",
        "            x, lambda x: self.self_attention_block(x, x, x, src_mask)\n",
        "        )\n",
        "\n",
        "        # Feed-forward residual\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xDNOT2-ilRZz"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Encoder\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()  # will operate on last dim (d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n"
      ],
      "metadata": {
        "id": "yu2Hx0fTlXwB"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        self_attention_block: MultiHeadAttentionBlock,\n",
        "        cross_attention_block: MultiHeadAttentionBlock,\n",
        "        feed_forward_block: FeedForwardBlock,\n",
        "        dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "\n",
        "        # 3 Residuals: Self-Attn, Cross-Attn, FeedForward\n",
        "        self.residual_connections = nn.ModuleList([\n",
        "            ResidualConnection(d_model, dropout),\n",
        "            ResidualConnection(d_model, dropout),\n",
        "            ResidualConnection(d_model, dropout)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "        # Masked self-attention\n",
        "        x = self.residual_connections[0](\n",
        "            x, lambda x: self.self_attention_block(x, x, x, tgt_mask)\n",
        "        )\n",
        "\n",
        "        # Cross-attention with encoder output\n",
        "        x = self.residual_connections[1](\n",
        "            x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)\n",
        "        )\n",
        "\n",
        "        # Feed-forward processing\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "J6fQYJ2_lbsz"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Decoder\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()  # Normalize final decoder output\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n"
      ],
      "metadata": {
        "id": "VDFrXTSkldKV"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Linear Projection Layer (Decoder Output -> Vocabulary)\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert to float32 for numerical stability before softmax\n",
        "        return torch.log_softmax(self.proj(x).float(), dim=-1)\n"
      ],
      "metadata": {
        "id": "OxeSJCyjlmRK"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Transformer Architecture\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: Encoder,\n",
        "        decoder: Decoder,\n",
        "        src_embed: InputEmbeddings,\n",
        "        tgt_embed: InputEmbeddings,\n",
        "        src_pos: PositionalEncoding,\n",
        "        tgt_pos: PositionalEncoding,\n",
        "        projection_layer: ProjectionLayer\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "    # NEW: forward method integrating the pipeline\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "        output = self.decode(encoder_output, src_mask, tgt, tgt_mask)\n",
        "        return self.project(output)\n"
      ],
      "metadata": {
        "id": "KAKzm30PlqFh"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building & Initializing Transformer\n",
        "def build_transformer(\n",
        "    src_vocab_size: int,\n",
        "    tgt_vocab_size: int,\n",
        "    src_seq_len: int,\n",
        "    tgt_seq_len: int,\n",
        "    d_model: int = 256,  # optimized for Colab GPU\n",
        "    N: int = 3,\n",
        "    h: int = 4,\n",
        "    dropout: float = 0.1,\n",
        "    d_ff: int = 512   # optimized for speed\n",
        ") -> Transformer:\n",
        "\n",
        "    # Embeddings\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Positional encodings\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Encoder stack\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_blocks.append(\n",
        "            EncoderBlock(d_model, encoder_self_attention, feed_forward, dropout)\n",
        "        )\n",
        "\n",
        "    # Decoder stack\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_blocks.append(\n",
        "            DecoderBlock(d_model, decoder_self_attention, decoder_cross_attention, feed_forward, dropout)\n",
        "        )\n",
        "\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    transformer = Transformer(\n",
        "        encoder,\n",
        "        decoder,\n",
        "        src_embed,\n",
        "        tgt_embed,\n",
        "        src_pos,\n",
        "        tgt_pos,\n",
        "        projection_layer\n",
        "    )\n",
        "\n",
        "    # Xavier initialization for stability\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n"
      ],
      "metadata": {
        "id": "jY0sEQU7luE9"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Tokenizer\n",
        "def build_tokenizer(config, ds, lang):\n",
        "\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "\n",
        "    # Ensure folder exists\n",
        "    tokenizer_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # If tokenizer doesn't already exist â†’ create one\n",
        "    if not tokenizer_path.exists():\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "        trainer = WordLevelTrainer(\n",
        "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
        "            min_frequency=1   # allow rare words when dataset is small\n",
        "        )\n",
        "\n",
        "        # Use dataset iterator\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "        print(f\"Trained tokenizer saved for language: {lang}\")\n",
        "\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "        print(f\"Loaded existing tokenizer: {tokenizer_path.name}\")\n",
        "\n",
        "    return tokenizer\n"
      ],
      "metadata": {
        "id": "_zdvFNyfl069"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for pair in ds:\n",
        "        if lang in pair:      # ensure key exists (e.g., \"en\" or \"de\")\n",
        "            yield pair[lang]\n"
      ],
      "metadata": {
        "id": "8LOYz9N4l6YR"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds(config):\n",
        "\n",
        "    # Load Multi30k dataset\n",
        "    ds_raw = load_dataset(\"bentrevett/multi30k\", split=\"train\")\n",
        "\n",
        "\n",
        "    # Tokenizers\n",
        "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Split dataset\n",
        "    split_ds = ds_raw.train_test_split(test_size=0.1, seed=42)\n",
        "    train_ds_raw = split_ds['train']\n",
        "    val_ds_raw = split_ds['test']\n",
        "\n",
        "    # Limit dataset size\n",
        "    train_limit = min(config[\"train_subset\"], len(train_ds_raw))\n",
        "    val_limit = min(config[\"val_subset\"], len(val_ds_raw))\n",
        "\n",
        "    train_ds_raw = train_ds_raw.select(range(train_limit))\n",
        "    val_ds_raw = val_ds_raw.select(range(val_limit))\n",
        "\n",
        "    # Create PyTorch datasets\n",
        "    train_ds = BilingualDataset(\n",
        "        train_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "        config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"]\n",
        "    )\n",
        "    val_ds = BilingualDataset(\n",
        "        val_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "        config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"]\n",
        "    )\n",
        "\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1)\n",
        "\n",
        "    print(f\"Train samples: {len(train_ds_raw)}, Val samples: {len(val_ds_raw)}\")\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
      ],
      "metadata": {
        "id": "jeGF7-vol-BN"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    # 1 = allowed, 0 = masked\n",
        "    mask = torch.tril(torch.ones(size, size)).unsqueeze(0)\n",
        "    return mask == 1\n"
      ],
      "metadata": {
        "id": "bNBurqSTmDe3"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
        "        self.eos_token = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
        "        self.pad_token = tokenizer_tgt.token_to_id(\"[PAD]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.ds[idx]\n",
        "        src_text = pair[self.src_lang]\n",
        "        tgt_text = pair[self.tgt_lang]\n",
        "\n",
        "        # Tokenize once only\n",
        "        enc_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Trim if too long\n",
        "        enc_tokens = enc_tokens[:self.seq_len - 2]\n",
        "        dec_tokens = dec_tokens[:self.seq_len - 1]\n",
        "\n",
        "        # Padding\n",
        "        enc_pad_count = self.seq_len - len(enc_tokens) - 2\n",
        "        dec_pad_count = self.seq_len - len(dec_tokens) - 1\n",
        "\n",
        "        # Encoder: [SOS] + src + [EOS] + PAD\n",
        "        encoder_input = torch.tensor(\n",
        "            [self.sos_token] + enc_tokens + [self.eos_token] +\n",
        "            [self.pad_token] * enc_pad_count,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Decoder input: [SOS] + tgt + PAD\n",
        "        decoder_input = torch.tensor(\n",
        "            [self.sos_token] + dec_tokens +\n",
        "            [self.pad_token] * dec_pad_count,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Labels: tgt + [EOS] + PAD\n",
        "        label = torch.tensor(\n",
        "            dec_tokens + [self.eos_token] +\n",
        "            [self.pad_token] * dec_pad_count,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        # Masks\n",
        "        encoder_mask = (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0)\n",
        "        decoder_mask = (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0) \\\n",
        "                       & causal_mask(decoder_input.size(0))\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,\n",
        "            \"decoder_input\": decoder_input,\n",
        "            \"encoder_mask\": encoder_mask,\n",
        "            \"decoder_mask\": decoder_mask,\n",
        "            \"label\": label,\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "UFFcG_jxmHbq"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Encode source\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "    # Decoder starts with <SOS>\n",
        "    decoder_input = torch.tensor([[sos_idx]], device=device)\n",
        "\n",
        "    while decoder_input.size(1) < max_len:\n",
        "\n",
        "        # Causal + padding mask for decode steps\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).to(device)\n",
        "\n",
        "        # Decode\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # Project last token only\n",
        "        logits = model.project(out[:, -1])\n",
        "        next_token = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Append token\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, next_token.unsqueeze(0)], dim=1\n",
        "        )\n",
        "\n",
        "        # Stop if EOS generated\n",
        "        if next_token.item() == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n"
      ],
      "metadata": {
        "id": "7UBddFTImMTK"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device, beam_size=5):\n",
        "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
        "    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
        "\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "    sequences = [[torch.tensor([sos_idx], device=device), 0.0]]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            if seq[-1].item() == eos_idx:\n",
        "                all_candidates.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            seq_input = seq.unsqueeze(0)\n",
        "            mask = causal_mask(seq_input.size(1)).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                decoder_output = model.decode(encoder_output, source_mask, seq_input, mask)\n",
        "                logits = model.project(decoder_output[:, -1])\n",
        "                probabilities = torch.log_softmax(logits, dim=-1).squeeze(0)\n",
        "\n",
        "            # Top beam_size candidates\n",
        "            topk_probs, topk_ids = torch.topk(probabilities, beam_size)\n",
        "\n",
        "            for prob, idx in zip(topk_probs, topk_ids):\n",
        "                candidate = torch.cat([seq, idx.unsqueeze(0)])\n",
        "                all_candidates.append((candidate, score + prob.item()))\n",
        "\n",
        "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_size]\n",
        "\n",
        "    return sequences[0][0]\n"
      ],
      "metadata": {
        "id": "gW5zMIrWww9D"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on validation dataset\n",
        "#switched to beam search for better results\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, num_examples=2, beam_size=5):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            # Validation loader must have batch_size = 1\n",
        "            assert encoder_input.size(0) == 1\n",
        "\n",
        "            # Use Beam Search instead of greedy\n",
        "            model_out = beam_search_decode(\n",
        "                model, encoder_input, encoder_mask,\n",
        "                tokenizer_src, tokenizer_tgt,\n",
        "                max_len, device,\n",
        "                beam_size=beam_size\n",
        "            )\n",
        "\n",
        "            # Decode text\n",
        "            source_text = batch['src_text'][0]\n",
        "            target_text = batch['tgt_text'][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.tolist())\n",
        "\n",
        "            # Display results\n",
        "            print_msg(\"-\" * console_width)\n",
        "            print_msg(f\"SOURCE:    {source_text}\")\n",
        "            print_msg(f\"TARGET:    {target_text}\")\n",
        "            print_msg(f\"PREDICTED: {model_out_text}\")\n",
        "\n",
        "            if count >= num_examples:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "W1jUoSs6mQkN"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len, device):\n",
        "    model = build_transformer(\n",
        "        vocab_src_len,\n",
        "        vocab_tgt_len,\n",
        "        config[\"seq_len\"],\n",
        "        config[\"seq_len\"],\n",
        "        d_model=config[\"d_model\"],\n",
        "        N=config[\"num_layers\"],\n",
        "        h=config[\"num_heads\"],\n",
        "        dropout=config[\"dropout\"],\n",
        "        d_ff=config[\"d_ff\"]\n",
        "    )\n",
        "\n",
        "    return model.to(device)\n"
      ],
      "metadata": {
        "id": "j_QytAeLmYCO"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 4,\n",
        "        \"num_epochs\": 30,\n",
        "        \"lr\": 1e-4,\n",
        "\n",
        "        \"seq_len\": 80,\n",
        "\n",
        "        \"d_model\": 256,\n",
        "        \"num_layers\": 3,\n",
        "        \"num_heads\": 4,\n",
        "        \"dropout\": 0.1,\n",
        "        \"d_ff\": 512,\n",
        "\n",
        "        # Change target language to German\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"de\",\n",
        "\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": None,\n",
        "        \"tokenizer_file\": \"tokenizers/tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel_en_de\",\n",
        "\n",
        "        \"train_subset\": 5000,\n",
        "        \"val_subset\": 500,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8SkqRPl6mby1"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\" Using device: {device}\")\n",
        "\n",
        "    # Ensure model + tokenizer folders exist\n",
        "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
        "    Path(\"tokenizers\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load dataset + tokenizers\n",
        "    train_dl, val_dl, tok_src, tok_tgt = get_ds(config)\n",
        "\n",
        "    # Build model\n",
        "    model = build_transformer(\n",
        "        tok_src.get_vocab_size(),\n",
        "        tok_tgt.get_vocab_size(),\n",
        "        config[\"seq_len\"],\n",
        "        config[\"seq_len\"],\n",
        "        d_model=config[\"d_model\"],\n",
        "        N=config[\"num_layers\"],\n",
        "        h=config[\"num_heads\"],\n",
        "        dropout=config[\"dropout\"],\n",
        "        d_ff=config[\"d_ff\"],\n",
        "    ).to(device)\n",
        "\n",
        "    writer = SummaryWriter(config[\"experiment_name\"])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    pad_id = tok_tgt.token_to_id(\"[PAD]\")\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=0.1)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if config[\"preload\"]:\n",
        "        ckpt = get_weights_file_path(config, config[\"preload\"])\n",
        "        print(f\"Loading checkpoint: {ckpt}\")\n",
        "        state = torch.load(ckpt, map_location=device)\n",
        "        model.load_state_dict(state[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "        global_step = state[\"global_step\"]\n",
        "        initial_epoch = state[\"epoch\"] + 1\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
        "        model.train()\n",
        "        loop = tqdm(train_dl, desc=f\"Epoch {epoch:02d}\")\n",
        "\n",
        "        for batch in loop:\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)\n",
        "            decoder_input = batch[\"decoder_input\"].to(device)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "            decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "            label = batch[\"label\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                enc_out = model.encode(encoder_input, encoder_mask)\n",
        "                dec_out = model.decode(enc_out, encoder_mask, decoder_input, decoder_mask)\n",
        "                proj = model.project(dec_out)\n",
        "\n",
        "                loss = loss_fn(\n",
        "                    proj.view(-1, tok_tgt.get_vocab_size()),\n",
        "                    label.view(-1)\n",
        "                )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Validation\n",
        "        run_validation(\n",
        "            model, val_dl,\n",
        "            tok_src, tok_tgt,\n",
        "            config[\"seq_len\"],\n",
        "            device,\n",
        "            lambda msg: loop.write(msg)\n",
        "        )\n",
        "\n",
        "        # Save each epoch\n",
        "        ckpt_path = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"global_step\": global_step\n",
        "        }, ckpt_path)\n",
        "        print(f\" Saved checkpoint: {ckpt_path}\")\n"
      ],
      "metadata": {
        "id": "EPGl1y_HmgwG"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n"
      ],
      "metadata": {
        "id": "xzXPiV4WmsV9"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings('ignore')\n",
        "    torch.cuda.empty_cache()  # optional GPU mem cleanup\n",
        "\n",
        "    config = get_config()\n",
        "    train_model(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "turtJ--rm9Zr",
        "outputId": "5dc86c9f-d6db-4ab2-e6d3-24ac587b8a5e"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš¡ Using device: cuda\n",
            "Trained tokenizer saved for language: en\n",
            "Trained tokenizer saved for language: de\n",
            "Train samples: 5000, Val samples: 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:09<00:00, 18.07it/s, loss=6.0254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Ein Mann in einem in einem , einem , und , einem , .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Mann in einem in einem , einem , und , einem , .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_00.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:08<00:00, 18.34it/s, loss=5.0658]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Ein Mann in einem Hemd in einem roten roten weiÃŸen weiÃŸen blauen StraÃŸe .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Mann in einem Hemd in einem roten roten weiÃŸen blauen StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_01.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:07<00:00, 18.42it/s, loss=5.0579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau in einem blauen Hemd , wÃ¤hrend ein Hund , die StraÃŸe , die StraÃŸe .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Eine Frau in einem Hemd , wÃ¤hrend ein Hund , die StraÃŸe , die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_02.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 03: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:07<00:00, 18.47it/s, loss=5.0150]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau in einem roten Hemd und ein MÃ¤dchen , wÃ¤hrend ein MÃ¤dchen , wÃ¤hrend ein MÃ¤dchen , wÃ¤hrend eine Frau .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Eine Frau in einem roten Hemd , wÃ¤hrend die , wÃ¤hrend die von der StraÃŸe , die von der StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_03.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 04: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:08<00:00, 18.33it/s, loss=5.4357]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und ein MÃ¤dchen und ein MÃ¤dchen , wÃ¤hrend ein MÃ¤dchen und ein MÃ¤dchen und ein Hund .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Eine Gruppe von Menschen , wÃ¤hrend die die von der StraÃŸe , wÃ¤hrend die StraÃŸe , wÃ¤hrend die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_04.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 19.01it/s, loss=4.8744]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Gruppe von Menschen und ein MÃ¤dchen , wÃ¤hrend ein MÃ¤dchen und ein MÃ¤dchen in der StraÃŸe .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Eine Gruppe von Menschen steht auf der StraÃŸe auf der StraÃŸe , die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_05.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 06: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 19.10it/s, loss=4.6609]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und eine Frau und eine Frau , wÃ¤hrend eine Frau in der StraÃŸe .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein kleines MÃ¤dchen , die auf der StraÃŸe , wÃ¤hrend andere von der StraÃŸe , wÃ¤hrend andere von der StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_06.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 07: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.67it/s, loss=4.9283]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und eine Frau und eine Frau in der Luft und hÃ¤lt .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein kleiner Junge sitzt auf der StraÃŸe , wÃ¤hrend der StraÃŸe , wÃ¤hrend der StraÃŸe , der StraÃŸe von der StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_07.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 08: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:04<00:00, 19.37it/s, loss=4.6031]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und eine Frau , die auf dem Boden und hÃ¤lt sich auf dem Boden .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht auf der StraÃŸe , wÃ¤hrend andere von der StraÃŸe , der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_08.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 09: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.79it/s, loss=5.0508]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und eine Frau , die auf dem Boden und schaut auf dem Boden .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht auf der StraÃŸe , wÃ¤hrend andere von der StraÃŸe von der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_09.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:07<00:00, 18.52it/s, loss=3.8915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und eine Frau , die auf dem Boden und schaut auf dem Boden .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht auf der StraÃŸe , wÃ¤hrend der StraÃŸe von der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_10.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.82it/s, loss=4.4518]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau und ein kleines MÃ¤dchen , die am Strand und schaut auf dem Boden .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht auf der StraÃŸe , wÃ¤hrend er von der StraÃŸe von der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_11.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:07<00:00, 18.53it/s, loss=4.0871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau in einem weiÃŸen Oberteil und Shorts lÃ¤uft am Strand .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor der StraÃŸe , wÃ¤hrend er von der StraÃŸe von der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_12.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.90it/s, loss=3.3596]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Frau in einem weiÃŸen Oberteil und Shorts lÃ¤uft am Strand .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht auf der StraÃŸe , der von der von der StraÃŸe steht vor der StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_13.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 18.98it/s, loss=4.3476]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame und ein schwarz - schwarz - schwarz - Shirt lÃ¤uft am Strand .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor dem Kopf von der StraÃŸe von der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_14.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 19.12it/s, loss=3.9654]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame und ein schwarz - schwarz - Shirt lÃ¤uft am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor dem Gehweg , der von der von der StraÃŸe eines Autos .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_15.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.78it/s, loss=3.8400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in blauen Shorts und Shorts lÃ¤uft am Strand am Strand .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor dem Kopf vor der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_16.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:07<00:00, 18.56it/s, loss=4.1050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in blau - Shorts und Shorts lÃ¤uft am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor der StraÃŸe entlang und hat die StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_17.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 19.10it/s, loss=4.3211]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts lÃ¤uft am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor dem Kopf vor der StraÃŸe entlang .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_18.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.75it/s, loss=3.5088]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts lÃ¤uft am Strand am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor der StraÃŸe und wird von BÃ¤umen Ã¼ber die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_19.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.91it/s, loss=3.2951]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts lÃ¤uft am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar steht vor dem Kopf vor der StraÃŸe , der von denen eines Autos die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_20.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.91it/s, loss=4.1889]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts lÃ¤uft am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar kÃ¼sst sich vor dem Kopf vor der StraÃŸe , das durch den BÃ¤umen steht .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_21.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 19.12it/s, loss=3.4309]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts blickt am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar genieÃŸt das Bild vor dem Kopf vor der StraÃŸe eines Autos .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_22.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.92it/s, loss=2.9141]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts blickt am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar kÃ¼sst sich vor einem Auto vor der StraÃŸe Ã¼ber die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_23.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:06<00:00, 18.71it/s, loss=3.3804]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts geht am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar kÃ¼sst sich vor einem Auto vor der StraÃŸe , das durch den Wald .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_24.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 19.11it/s, loss=2.9345]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts blickt am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar bereitet sich vor dem Kopf vor der StraÃŸe vor einer Reihe , im Hintergrund zu sehen sind .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_25.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:04<00:00, 19.24it/s, loss=2.7777]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame mit grÃ¼nen Shorts und Shorts blickt am Strand in die Luft .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar kÃ¼sst sich vor dem Kopf vor der StraÃŸe Ã¼ber die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_26.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 18.97it/s, loss=2.3822]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts blickt am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar schneidet das Bild vor der RÃ¼ckseite eines Autos Ã¼ber die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_27.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:04<00:00, 19.37it/s, loss=2.7209]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame in grÃ¼nen Shorts und Shorts geht am Strand entlang .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar kÃ¼sst sich vor dem Kopf vor dem RÃ¼cken eines Autos Ã¼ber die StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_28.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [01:05<00:00, 18.99it/s, loss=2.1333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A lady wearing green and white shorts and top is on the beach clapping her hands.\n",
            "TARGET:    Eine Dame mit grÃ¼n-weiÃŸen Shorts und Oberteil ist auf dem Strand und klatscht in die HÃ¤nde.\n",
            "PREDICTED: Eine Dame mit grÃ¼nen Shorts und Shorts blickt Ã¼ber die HÃ¤nde am Strand .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE:    A couple takes their own picture in front of the Arc De TRiomphe, from across the street.\n",
            "TARGET:    Ein Paar macht auf der anderen StraÃŸenseite des Arc de Triomphe ein Bild von sich.\n",
            "PREDICTED: Ein Paar kÃ¼sst sich vor einem alten Paar vor der StraÃŸe .\n",
            "ðŸ’¾ Saved checkpoint: weights/tmodel_29.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load config\n",
        "config = get_config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load tokenizers\n",
        "train_dl, val_dl, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "# Build model\n",
        "vocab_src_len = tokenizer_src.get_vocab_size()\n",
        "vocab_tgt_len = tokenizer_tgt.get_vocab_size()\n",
        "model = get_model(config, vocab_src_len, vocab_tgt_len, device)\n",
        "\n",
        "# Load latest checkpoint\n",
        "CHECKPOINT_EPOCH = \"29\"\n",
        "checkpoint_path = get_weights_file_path(config, CHECKPOINT_EPOCH)\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "print(f\"Loaded model checkpoint: {checkpoint_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STFUyjDpQ0ym",
        "outputId": "358da78e-b354-4517-950d-85d380b6340c"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing tokenizer: tokenizer_en.json\n",
            "Loaded existing tokenizer: tokenizer_de.json\n",
            "Train samples: 5000, Val samples: 500\n",
            "Loaded model checkpoint: weights/tmodel_29.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, tokenizer_src, tokenizer_tgt, sentence, device, max_len=80):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize source sentence\n",
        "    enc_tokens = tokenizer_src.encode(sentence).ids\n",
        "    enc_tokens = enc_tokens[:max_len - 2]  # safety clipping\n",
        "    src = torch.tensor(\n",
        "        [tokenizer_src.token_to_id(\"[SOS]\")] + enc_tokens + [tokenizer_src.token_to_id(\"[EOS]\")]\n",
        "    ).unsqueeze(0).to(device)\n",
        "\n",
        "    # Create encoder mask\n",
        "    src_mask = (src != tokenizer_src.token_to_id(\"[PAD]\")).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "    # Run encoder\n",
        "    with torch.no_grad():\n",
        "        enc_out = model.encode(src, src_mask)\n",
        "\n",
        "    # Greedy decode loop\n",
        "    tgt = torch.tensor([[tokenizer_tgt.token_to_id(\"[SOS]\")]]).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_mask = causal_mask(tgt.size(1)).to(device)\n",
        "        with torch.no_grad():\n",
        "            dec_out = model.decode(enc_out, src_mask, tgt, tgt_mask)\n",
        "            logits = model.project(dec_out[:, -1])\n",
        "            next_token = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        tgt = torch.cat([tgt, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer_tgt.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "    # Convert token IDs â†’ text\n",
        "    out_tokens = tgt.squeeze().tolist()\n",
        "    return tokenizer_tgt.decode(out_tokens)\n"
      ],
      "metadata": {
        "id": "LXn1kdn5nFF7"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "sentence = \"the children are playing in the park.\"\n",
        "translated = translate_sentence(model, tokenizer_src, tokenizer_tgt, sentence, device)\n",
        "print(\"INPUT :\", sentence)\n",
        "print(\"OUTPUT:\", translated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXlxrQXCQVw3",
        "outputId": "15a1b730-cb97-4397-895a-3baac6aeb3f9"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT : the children are playing in the park.\n",
            "OUTPUT: Kinder spielen im Park .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXYZ8eWDQrBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}